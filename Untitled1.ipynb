{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zza6ciktSGy1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "import shutil\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "import albumentations as A\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from IPython.display import YouTubeVideo\n",
        "import sys\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_in_color(txt_msg,fore_tupple=(0,255,255),back_tupple=(100,100,100)):\n",
        "    rf,gf,bf=fore_tupple\n",
        "    rb,gb,bb=back_tupple\n",
        "    msg='{0}' + txt_msg\n",
        "    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m'\n",
        "    print(msg .format(mat), flush=True)\n",
        "    print('\\33[0m', flush=True)\n",
        "    return"
      ],
      "metadata": {
        "id": "SM4WY1hhXFS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_label_count (df, plot_title):\n",
        "    column='labels'\n",
        "    xaxis_label='CLASS'\n",
        "    yaxis_label='IMAGE COUNT'\n",
        "    vcounts=df[column].value_counts()\n",
        "    labels=vcounts.keys().tolist()\n",
        "    values=vcounts.tolist()\n",
        "    lcount=len(labels)\n",
        "    if lcount>55:\n",
        "        print_in_color('The number of labels is >55, no plot will be produced')\n",
        "    else:\n",
        "        width=lcount * 4\n",
        "        width=np.min([width, 20])\n",
        "        plt.figure(figsize=(width,5))\n",
        "        form = {'family': 'serif', 'color': 'blue', 'size': 25}\n",
        "        plt.bar(labels, values)\n",
        "        plt.title(plot_title, fontsize= 24, color='blue')\n",
        "        plt.xticks(rotation=90, fontsize=18)\n",
        "        plt.yticks(fontsize=18)\n",
        "        plt.xlabel(xaxis_label, fontdict=form)\n",
        "        plt.ylabel(yaxis_label, fontdict=form)\n",
        "        if lcount >=8:\n",
        "            rotation='vertical'\n",
        "        else:\n",
        "            rotation='horizontal'\n",
        "        for i in range(lcount):\n",
        "            plt.text(i, values[i]/2, str(values[i]),fontsize=12, rotation=rotation, color='yellow', ha='center')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "X9EYYVL5XVMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataframes(train_dir,test_dir, val_dir, limiter):\n",
        "    bad_images=[]\n",
        "    if test_dir == None and val_dir==None:\n",
        "        dirlist=[train_dir]\n",
        "        names = ['train']\n",
        "    elif test_dir == None:\n",
        "        dirlist=[train_dir,  val_dir]\n",
        "        names=['train', 'valid']\n",
        "    elif val_dir == None:\n",
        "        dirlist=[train_dir,  test_dir]\n",
        "        names=['train', 'test']\n",
        "    else:\n",
        "        dirlist=[train_dir, test_dir, val_dir]\n",
        "        names=['train','test', 'valid']\n",
        "    ht=0\n",
        "    wt=0\n",
        "    total_good_files=0\n",
        "    zipdir=zip(names, dirlist)\n",
        "    for name,d in zipdir:\n",
        "        filepaths=[]\n",
        "        labels=[]\n",
        "        classlist=sorted(os.listdir(d) )\n",
        "        for klass in classlist:\n",
        "            good_file_count=0\n",
        "            classpath=os.path.join(d, klass)\n",
        "            if os.path.isdir(classpath):\n",
        "                flist=sorted(os.listdir(classpath))\n",
        "                if limiter != None:\n",
        "                    if limiter <len(flist):\n",
        "                        flist=np.random.choice(flist, limiter, replace=False)\n",
        "                desc=f'{name:6s}-{klass:25s}'\n",
        "                for f in tqdm(flist, ncols=130,desc=desc, unit='files', colour='blue'):\n",
        "                    fpath=os.path.join(classpath,f)\n",
        "                    try:\n",
        "                        img=cv2.imread(fpath)\n",
        "                        h=img.shape[0]\n",
        "                        w=img.shape[1]\n",
        "                        ht +=h\n",
        "                        wt += w\n",
        "                        good_file_count +=1\n",
        "                        total_good_files +=1\n",
        "                        filepaths.append(fpath)\n",
        "                        labels.append(klass)\n",
        "\n",
        "                    except:\n",
        "                        bad_images.append(fpath)\n",
        "        Fseries=pd.Series(filepaths, name='filepaths')\n",
        "        Lseries=pd.Series(labels, name='labels')\n",
        "        df=pd.concat([Fseries, Lseries], axis=1)\n",
        "        if name =='valid':\n",
        "            valid_df=df\n",
        "        elif name == 'test':\n",
        "            test_df=df\n",
        "        else:\n",
        "            if test_dir == None and val_dir == None:\n",
        "                pdf=df\n",
        "                train_df, dummy_df=train_test_split(pdf, train_size=.8, shuffle=True, random_state=123, stratify=pdf['labels'])\n",
        "                valid_df, test_df=train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify=dummy_df['labels'])\n",
        "            elif test_dir == None:\n",
        "                pdf=df\n",
        "                train_df,test_df=train_test_split(pdf, train_size=.8, shuffle=True, random_state=123, stratify=pdf['labels'])\n",
        "            elif val_dir == None:\n",
        "                pdf=df\n",
        "                train_df,valid_df=train_test_split(pdf, train_size=.8, shuffle=True, random_state=123, stratify=pdf['labels'])\n",
        "            else:\n",
        "                train_df= df\n",
        "    classes=sorted(train_df['labels'].unique())\n",
        "    class_count=len(classes)\n",
        "    sample_df=train_df.sample(n=100, replace=False)\n",
        "    have=int(ht/total_good_files)\n",
        "    wave=int(wt/total_good_files)\n",
        "    aspect_ratio=have/wave\n",
        "    print('number of classes in processed dataset= ', class_count)\n",
        "    counts=list(train_df['labels'].value_counts())\n",
        "    print('the maximum files in any class in train_df is ', max(counts), '  the minimum files in any class in train_df is ', min(counts))\n",
        "    print('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))\n",
        "    print('average image height= ', have, '  average image width= ', wave, ' aspect ratio h/w= ', aspect_ratio)\n",
        "    if len(bad_images) == 0:\n",
        "        print_in_color('All image files were properly processed and used in the dataframes')\n",
        "    else:\n",
        "        print_in_color(f'the are {len(bad_images)} bad image files and {total_good_files} proper image files in the dataset')\n",
        "        for f in bad_images:\n",
        "            print (f)\n",
        "    plot_title='Images per Label in train set'\n",
        "    plot_label_count (train_df,  plot_title)\n",
        "    return train_df, test_df, valid_df, classes, class_count, max(counts), min(counts), have, wave"
      ],
      "metadata": {
        "id": "Q1QU54KeMBuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim(df, max_samples, min_samples, column):\n",
        "    df=df.copy()\n",
        "    classes=df[column].unique()\n",
        "    class_count=len(classes)\n",
        "    length=len(df)\n",
        "    print ('dataframe initially is of length ',length, ' with ', class_count, ' classes')\n",
        "    groups=df.groupby(column)\n",
        "    trimmed_df = pd.DataFrame(columns = df.columns)\n",
        "    groups=df.groupby(column)\n",
        "    for label in df[column].unique():\n",
        "        group=groups.get_group(label)\n",
        "        count=len(group)\n",
        "        if count > max_samples:\n",
        "            sampled_group=group.sample(n=max_samples, random_state=123,axis=0)\n",
        "            trimmed_df=pd.concat([trimmed_df, sampled_group], axis=0)\n",
        "        else:\n",
        "            if count>=min_samples:\n",
        "                sampled_group=group\n",
        "                trimmed_df=pd.concat([trimmed_df, sampled_group], axis=0)\n",
        "    print('after trimming, the maximum samples in any class is now ',max_samples, ' and the minimum samples in any class is ', min_samples)\n",
        "    classes=trimmed_df[column].unique()\n",
        "    class_count=len(classes)\n",
        "    length=len(trimmed_df)\n",
        "    print ('the trimmed dataframe now is of length ',length, ' with ', class_count, ' classes')\n",
        "    return trimmed_df, classes, class_count\n"
      ],
      "metadata": {
        "id": "o4a6zLnHMWxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balance(df, n,column, working_dir, img_size):\n",
        "    def get_augmented_image(image):\n",
        "        width=int(image.shape[1]*.8)\n",
        "        height=int(image.shape[0]*.8)\n",
        "        transform= A.Compose([\n",
        "            A.HorizontalFlip(p=.5),\n",
        "            A.Rotate(limit=30, p=.25),\n",
        "            A.RandomBrightnessContrast(p=.5),\n",
        "            A.RandomGamma(p=.5),\n",
        "            A.RandomCrop(width=width, height=height, p=.25) ])\n",
        "        return transform(image=image)['image']\n",
        "    def dummy(image):\n",
        "        return image\n",
        "\n",
        "    df=df.copy()\n",
        "    print('Initial length of dataframe is ', len(df))\n",
        "    aug_dir=os.path.join(working_dir, 'aug')\n",
        "    if os.path.isdir(aug_dir):\n",
        "        shutil.rmtree(aug_dir)\n",
        "    os.mkdir(aug_dir)\n",
        "    for label in df[column].unique():\n",
        "        dir_path=os.path.join(aug_dir,label)\n",
        "        os.mkdir(dir_path)\n",
        "    total=0\n",
        "    groups=df.groupby(column)\n",
        "    for label in df[column].unique():\n",
        "        group=groups.get_group(label)\n",
        "        sample_count=len(group)\n",
        "        if sample_count< n:\n",
        "            aug_img_count=0\n",
        "            delta=n - sample_count\n",
        "            target_dir=os.path.join(aug_dir, label)\n",
        "            desc=f'augmenting class {label}'\n",
        "            for i in tqdm(range(delta), ncols=120, unit='files', colour='blue',desc=desc):\n",
        "                j= i % sample_count\n",
        "                img_path=group['filepaths'].iloc[j]\n",
        "                img=cv2.imread(img_path)\n",
        "                img=get_augmented_image(img)\n",
        "                fname=os.path.basename(img_path)\n",
        "                fname='aug' +str(i) +'-' +fname\n",
        "                dest_path=os.path.join(target_dir, fname)\n",
        "                cv2.imwrite(dest_path, img)\n",
        "                aug_img_count +=1\n",
        "            total +=aug_img_count\n",
        "    print('Total Augmented images created= ', total)\n",
        "\n",
        "    aug_fpaths=[]\n",
        "    aug_labels=[]\n",
        "    classlist=sorted(os.listdir(aug_dir))\n",
        "    for klass in classlist:\n",
        "        classpath=os.path.join(aug_dir, klass)\n",
        "        flist=sorted(os.listdir(classpath))\n",
        "        for f in flist:\n",
        "            fpath=os.path.join(classpath,f)\n",
        "            aug_fpaths.append(fpath)\n",
        "            aug_labels.append(klass)\n",
        "    Fseries=pd.Series(aug_fpaths, name='filepaths')\n",
        "    Lseries=pd.Series(aug_labels, name='labels')\n",
        "    aug_df=pd.concat([Fseries, Lseries], axis=1)\n",
        "    df=pd.concat([df,aug_df], axis=0).reset_index(drop=True)\n",
        "    print('Length os augmented dataframe is now ', len(df))\n",
        "    return df"
      ],
      "metadata": {
        "id": "81flzDbhNe2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gens(batch_size, ycol, train_df, test_df, valid_df, img_size):\n",
        "    trgen=ImageDataGenerator(horizontal_flip=True)\n",
        "    t_and_v_gen=ImageDataGenerator()\n",
        "    msg='{0:70s} for train generator'.format(' ')\n",
        "    print(msg, '\\r', end='')\n",
        "    train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col=ycol, target_size=img_size,\n",
        "                                       class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
        "    msg='{0:70s} for valid generator'.format(' ')\n",
        "    print(msg, '\\r', end='')\n",
        "    valid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col=ycol, target_size=img_size,\n",
        "                                       class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    length=len(test_df)\n",
        "    test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]\n",
        "    test_steps=int(length/test_batch_size)\n",
        "    msg='{0:70s} for test generator'.format(' ')\n",
        "    print(msg, '\\r', end='')\n",
        "    test_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col=ycol, target_size=img_size,\n",
        "                                       class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
        "\n",
        "    classes=list(train_gen.class_indices.keys())\n",
        "    class_indices=list(train_gen.class_indices.values())\n",
        "    class_count=len(classes)\n",
        "    labels=test_gen.labels\n",
        "    return train_gen, test_gen, valid_gen, test_steps"
      ],
      "metadata": {
        "id": "PbKEO8UJNfV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image_samples(gen ):\n",
        "    msg='Below are some example training images'\n",
        "    print_in_color(msg)\n",
        "    t_dict=gen.class_indices\n",
        "    classes=list(t_dict.keys())\n",
        "    images,labels=next(gen)\n",
        "    plt.figure(figsize=(25, 25))\n",
        "    length=len(labels)\n",
        "    if length<25:\n",
        "        r=length\n",
        "    else:\n",
        "        r=25\n",
        "    for i in range(r):\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        image=images[i] /255\n",
        "        plt.imshow(image)\n",
        "        index=np.argmax(labels[i])\n",
        "        class_name=classes[index]\n",
        "        plt.title(class_name, color='blue', fontsize=18)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aa9JgPxdNfp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def F1_score(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "metadata": {
        "id": "KE3SNtPVNgGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(img_size, lr, mod_num=3):\n",
        "    img_shape=(img_size[0], img_size[1], 3)\n",
        "    if mod_num == 0:\n",
        "        base_model=tf.keras.applications.efficientnet.EfficientNetB0(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
        "        msg='Created EfficientNet B0 model'\n",
        "    elif mod_num == 3:\n",
        "        base_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
        "        msg='Created EfficientNet B3 model'\n",
        "    elif mod_num == 5:\n",
        "        base_model=tf.keras.applications.efficientnet.EfficientNetB5(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
        "        msg='Created EfficientNet B5 model'\n",
        "\n",
        "    else:\n",
        "        base_model=tf.keras.applications.efficientnet.EfficientNetB7(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
        "        msg='Created EfficientNet B7 model'\n",
        "\n",
        "    base_model.trainable=True\n",
        "    x=base_model.output\n",
        "    x=BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
        "    x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
        "                    bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
        "    x=Dropout(rate=.4, seed=123)(x)\n",
        "    output=Dense(class_count, activation='softmax')(x)\n",
        "    model=Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy', F1_score, 'AUC'])\n",
        "    msg=msg + f' with initial learning rate set to {lr}'\n",
        "    print_in_color(msg)\n",
        "    return model"
      ],
      "metadata": {
        "id": "8k6WOPjnOGIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LR_ASK(keras.callbacks.Callback):\n",
        "    def __init__ (self, model, epochs,  ask_epoch, dwell=True, factor=.4):\n",
        "        super(LR_ASK, self).__init__()\n",
        "        self.model=model\n",
        "        self.ask_epoch=ask_epoch\n",
        "        self.epochs=epochs\n",
        "        self.ask=True\n",
        "        self.lowest_vloss=np.inf\n",
        "        self.lowest_aloss=np.inf\n",
        "        self.best_weights=self.model.get_weights()\n",
        "        self.best_epoch=1\n",
        "        self.plist=[]\n",
        "        self.alist=[]\n",
        "        self.dwell= dwell\n",
        "        self.factor=factor\n",
        "\n",
        "    def get_list(self):\n",
        "        return self.plist, self.alist\n",
        "    def on_train_begin(self, logs=None):\n",
        "        if self.ask_epoch == 0:\n",
        "            print('you set ask_epoch = 0, ask_epoch will be set to 1', flush=True)\n",
        "            self.ask_epoch=1\n",
        "        if self.ask_epoch >= self.epochs:\n",
        "            print('ask_epoch >= epochs, will train for ', epochs, ' epochs', flush=True)\n",
        "            self.ask=False\n",
        "        if self.epochs == 1:\n",
        "            self.ask=False\n",
        "        else:\n",
        "            msg =f'Training will proceed until epoch {ask_epoch} then you will be asked to'\n",
        "            print_in_color(msg )\n",
        "            msg='enter H to halt training or enter an integer for how many more epochs to run then be asked again'\n",
        "            print_in_color(msg)\n",
        "            if self.dwell:\n",
        "                msg='learning rate will be automatically adjusted during training'\n",
        "                print_in_color(msg, (0,255,0))\n",
        "        self.start_time= time.time()\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        msg=f'loading model with weights from epoch {self.best_epoch}'\n",
        "        print_in_color(msg, (0,255,255))\n",
        "        self.model.set_weights(self.best_weights)\n",
        "        tr_duration=time.time() - self.start_time\n",
        "        hours = tr_duration // 3600\n",
        "        minutes = (tr_duration - (hours * 3600)) // 60\n",
        "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
        "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
        "        print_in_color (msg)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        vloss=logs.get('val_loss')\n",
        "        aloss=logs.get('loss')\n",
        "        if epoch >0:\n",
        "            deltav = self.lowest_vloss- vloss\n",
        "            pimprov=(deltav/self.lowest_vloss) * 100\n",
        "            self.plist.append(pimprov)\n",
        "            deltaa=self.lowest_aloss-aloss\n",
        "            aimprov=(deltaa/self.lowest_aloss) * 100\n",
        "            self.alist.append(aimprov)\n",
        "        else:\n",
        "            pimprov=0.0\n",
        "            aimprov=0.0\n",
        "        if vloss< self.lowest_vloss:\n",
        "            self.lowest_vloss=vloss\n",
        "            self.best_weights=self.model.get_weights()\n",
        "            self.best_epoch=epoch + 1\n",
        "            msg=f'\\n validation loss of {vloss:7.4f} is {pimprov:7.4f} % below lowest loss, saving weights from epoch {str(epoch + 1):3s} as best weights'\n",
        "            print_in_color(msg, (0,255,0))\n",
        "        else:\n",
        "            pimprov=abs(pimprov)\n",
        "            msg=f'\\n validation loss of {vloss:7.4f} is {pimprov:7.4f} % above lowest loss of {self.lowest_vloss:7.4f} keeping weights from epoch {str(self.best_epoch)} as best weights'\n",
        "            print_in_color(msg, (255,255,0))\n",
        "            if self.dwell:\n",
        "                lr=float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "                new_lr=lr * self.factor\n",
        "                msg=f'learning rate was automatically adjusted from {lr:8.6f} to {new_lr:8.6f}, model weights set to best weights'\n",
        "                print_in_color(msg)\n",
        "                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
        "                self.model.set_weights(self.best_weights)\n",
        "\n",
        "        if aloss< self.lowest_aloss:\n",
        "            self.lowest_aloss=aloss\n",
        "        if self.ask:\n",
        "            if epoch + 1 ==self.ask_epoch:\n",
        "                msg='\\n Enter H to end training or  an integer for the number of additional epochs to run then ask again'\n",
        "                print_in_color(msg)\n",
        "                ans=input()\n",
        "\n",
        "                if ans == 'H' or ans =='h' or ans == '0':\n",
        "                    msg=f'you entered {ans},  Training halted on epoch {epoch+1} due to user input\\n'\n",
        "                    print_in_color(msg)\n",
        "                    self.model.stop_training = True\n",
        "                else:\n",
        "                    self.ask_epoch += int(ans)\n",
        "                    if self.ask_epoch > self.epochs:\n",
        "                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n",
        "                    else:\n",
        "                        msg=f'you entered {ans} Training will continue to epoch {self.ask_epoch}'\n",
        "                        print_in_color(msg)\n",
        "                        if self.dwell==False:\n",
        "                            lr=float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "                            msg=f'current LR is  {lr:8.6f}  hit enter to keep  this LR or enter a new LR'\n",
        "                            print_in_color(msg)\n",
        "                            ans=input(' ')\n",
        "                            if ans =='':\n",
        "                                msg=f'keeping current LR of {lr:7.5f}'\n",
        "                                print_in_color(msg)\n",
        "                            else:\n",
        "                                new_lr=float(ans)\n",
        "                                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
        "                                msg=f' changing LR to {ans}'\n",
        "                                print_in_color(msg)"
      ],
      "metadata": {
        "id": "0G70tfRcOGX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tr_plot(tr_data):\n",
        "    start_epoch=0\n",
        "    tacc=tr_data.history['accuracy']\n",
        "    tloss=tr_data.history['loss']\n",
        "    vacc=tr_data.history['val_accuracy']\n",
        "    vloss=tr_data.history['val_loss']\n",
        "    tf1=tr_data.history['F1_score']\n",
        "    vf1=tr_data.history['val_F1_score']\n",
        "    Epoch_count=len(tacc)+ start_epoch\n",
        "    Epochs=[]\n",
        "    for i in range (start_epoch ,Epoch_count):\n",
        "        Epochs.append(i+1)\n",
        "    index_loss=np.argmin(vloss)\n",
        "    val_lowest=vloss[index_loss]\n",
        "    index_acc=np.argmax(vacc)\n",
        "    acc_highest=vacc[index_acc]\n",
        "    indexf1=np.argmax(vf1)\n",
        "    vf1_highest=vf1[indexf1]\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
        "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
        "    f1_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
        "    fig,axes=plt.subplots(nrows=1, ncols=3, figsize=(25,10))\n",
        "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
        "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
        "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
        "    axes[0].scatter(Epochs, tloss, s=100, c='red')\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].set_xlabel('Epochs', fontsize=18)\n",
        "    axes[0].set_ylabel('Loss', fontsize=18)\n",
        "    axes[0].legend()\n",
        "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
        "    axes[1].scatter(Epochs, tacc, s=100, c='red')\n",
        "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
        "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
        "    axes[1].set_title('Training and Validation Accuracy')\n",
        "    axes[1].set_xlabel('Epochs', fontsize=18)\n",
        "    axes[1].set_ylabel('Accuracy', fontsize=18)\n",
        "    axes[1].legend()\n",
        "    axes[2].plot (Epochs,tf1,'r',label= 'Training F1 score')\n",
        "    axes[2].plot (Epochs,vf1,'g',label= 'Validation F1 score')\n",
        "    index_tf1=np.argmax(tf1)\n",
        "    tf1max=tf1[index_tf1]\n",
        "    index_vf1=np.argmax(vf1)\n",
        "    vf1max=vf1[index_vf1]\n",
        "    axes[2].scatter(index_vf1+1 +start_epoch,vf1max, s=150, c= 'blue', label=vc_label)\n",
        "    axes[2].scatter(Epochs, tf1, s=100, c='red')\n",
        "    axes[2].set_title('Training and Validation F1 score')\n",
        "    axes[2].set_xlabel('Epochs', fontsize=18)\n",
        "    axes[2].set_ylabel('F1  score', fontsize=18)\n",
        "    axes[2].legend()\n",
        "    plt.tight_layout\n",
        "    plt.show()\n",
        "    return"
      ],
      "metadata": {
        "id": "m8xAmT8lOGzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictor(test_gen):\n",
        "    y_pred= []\n",
        "    error_list=[]\n",
        "    error_pred_list = []\n",
        "    y_true=test_gen.labels\n",
        "    classes=list(test_gen.class_indices.keys())\n",
        "    class_count=len(classes)\n",
        "    errors=0\n",
        "    preds=model.predict(test_gen, verbose=1)\n",
        "    tests=len(preds)\n",
        "    for i, p in enumerate(preds):\n",
        "        pred_index=np.argmax(p)\n",
        "        true_index=test_gen.labels[i]\n",
        "        if pred_index != true_index:\n",
        "            errors=errors + 1\n",
        "            file=test_gen.filenames[i]\n",
        "            error_list.append(file)\n",
        "            error_class=classes[pred_index]\n",
        "            error_pred_list.append(error_class)\n",
        "        y_pred.append(pred_index)\n",
        "\n",
        "    acc=( 1-errors/tests) * 100\n",
        "    msg=f'there were {errors} errors in {tests} tests for an accuracy of {acc:6.2f}'\n",
        "    print_in_color(msg, (0,255,255), (100,100,100))\n",
        "    ypred=np.array(y_pred)\n",
        "    ytrue=np.array(y_true)\n",
        "    f1score=f1_score(ytrue, ypred, average='weighted')* 100\n",
        "    if class_count <=30:\n",
        "        cm = confusion_matrix(ytrue, ypred )\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)\n",
        "        plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n",
        "        plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "    clr = classification_report(y_true, y_pred, target_names=classes, digits= 4)\n",
        "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
        "    return errors, tests, error_list, error_pred_list, f1score"
      ],
      "metadata": {
        "id": "1Gu-ebmUOHBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_errors(error_list, dilimter):\n",
        "    if len(error_list) == 0:\n",
        "        print_in_color('There were no errors in predicting the test set')\n",
        "    else:\n",
        "        if len(error_list)>50:\n",
        "            print_in_color('There were over 50 misclassifications, the error list will not be printed')\n",
        "        else:\n",
        "            print ('Below is a list of test files that were miss classified \\n')\n",
        "            print ('{0:^50s}{1:^50s}'.format('Test File', ' Predicted as'))\n",
        "            for i in range(len(error_list)):\n",
        "                fpath=error_list[i]\n",
        "                split=fpath.split(delimiter)\n",
        "                slength=len(split)\n",
        "                f=split[slength-2]+ '-' + split[slength-1]\n",
        "                print(f'{f:^50s}{error_pred_list[i]:^50s}')"
      ],
      "metadata": {
        "id": "EYhsxFgUOHY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_misclassification(error_list, error_pred_list, test_gen, delimiter):\n",
        "    if len(error_list) == 0:\n",
        "        print_in_color('there were no errors in predicting the test images')\n",
        "    else:\n",
        "        if len(error_list)<10:\n",
        "            length=len(error_list)\n",
        "        else:\n",
        "            length = 10\n",
        "        msg='The images below show 10 misclassified test images on left and an example of an image in the  misclassified class'\n",
        "        print_in_color(msg)\n",
        "        test_files=test_gen.filenames\n",
        "        plt.figure(figsize=(15, length * 5))\n",
        "        for i in range(length):\n",
        "            fpath=error_list[i]\n",
        "            test_img=plt.imread(fpath)\n",
        "            pred_class= error_pred_list[i]\n",
        "            for f in test_gen.filenames:\n",
        "                split=list(f.split(delimiter))\n",
        "                klass=split[len(split)-2]\n",
        "                if klass == pred_class:\n",
        "                    pred_img_path= f\n",
        "            pred_img=plt.imread(pred_img_path)\n",
        "            for j in range(2):\n",
        "                k=i*2 + j + 1\n",
        "                plt.subplot(length, 2, k)\n",
        "                plt.axis('off')\n",
        "                if j == 0:\n",
        "                    plt.imshow(test_img)\n",
        "                    split=fpath.split(delimiter)\n",
        "                    slength=len(split)\n",
        "                    #print (split)\n",
        "                    title=split[slength-2]+ '-' + split[slength-1]\n",
        "                    title='TEST IMAGE\\n'+ title\n",
        "                    plt.title(title, color='blue', fontsize=16)\n",
        "                else:\n",
        "                    plt.imshow(pred_img)\n",
        "                    split=pred_img_path.split(delimiter)\n",
        "                    slength=len(split)\n",
        "                    title=split[slength-2]+ '-' + split[slength-1]\n",
        "                    title='PREDICTED CLASS EXAMPLE\\n'+ title\n",
        "                    plt.title(title, color='blue', fontsize=16)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "gWAZuNAPOHph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(subject, classes, img_size, f1score, working_dir):\n",
        "    name=subject + '-' + str(len(classes)) + '-(' + str(img_size[0]) + ' X ' + str(img_size[1]) + ')'\n",
        "    save_id=f'{name}-{f1score:5.2f}.h5'\n",
        "    model_save_loc=os.path.join(working_dir, save_id)\n",
        "    model.save(model_save_loc)\n",
        "    msg= f'model was saved as {model_save_loc}'\n",
        "    print_in_color(msg, (0,255,255), (100,100,100))"
      ],
      "metadata": {
        "id": "zwZSWuxqPINc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_image_size(have, wave):\n",
        "        if have<= 224 and wave <= 224:\n",
        "            img_size=(have, wave)\n",
        "        else:\n",
        "            if have>= wave:\n",
        "                img_size =(224, int (224*wave/have))\n",
        "            else:\n",
        "                img_size=(int(224* have/wave))\n",
        "            return img_size"
      ],
      "metadata": {
        "id": "gAy-tU7qPIlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_balance(train_df, img_size,max_samples, min_samples,  working_dir):\n",
        "    msg='enter the number of images you want to have in each class of the train data set'\n",
        "    max_images=int(input(msg))\n",
        "    msg='enter the minimum number of images a class must have to be included in the train data set'\n",
        "    min_images = int(input(msg))\n",
        "    train_df, classes, class_count=trim (train_df, max_images, min_images, 'labels')\n",
        "    train_df=balance(train_df, max_images, 'labels', working_dir, img_size)\n",
        "    plot_title='Images per Label after Auto Balance of train data set'\n",
        "    plot_label_count (train_df,  plot_title)\n",
        "    return train_df"
      ],
      "metadata": {
        "id": "HkdAOeqLPI4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_balance(train_df,img_size, max_samples, min_samples, working_dir):\n",
        "    msg = f' maximum images in a class is {max_samples}, minimum images in a class is {min_samples}'\n",
        "    print_in_color(msg)\n",
        "    msg=' if you want to trim the train set so no class has more than n images\\n enter the maximun number of images allowed in a class or press enter to not trim'\n",
        "    ans=input(msg)\n",
        "    if ans == '':\n",
        "        max_images= max_samples\n",
        "    else:\n",
        "        max_images=int(ans)\n",
        "    msg=' if you want to eliminate classes that have less than a minimum number of images\\n enter the minimum number of images a class must have to be included in the dataset or press enter to include all classes'\n",
        "    ans=input(msg)\n",
        "    if ans == '':\n",
        "        min_images=min_samples\n",
        "    else:\n",
        "        min_images=int(ans)\n",
        "    train_df, classes, class_count=trim(train_df, max_images, min_images, 'labels')\n",
        "    plot_title='Images per Label after trimming the dataset'\n",
        "    plot_label_count (train_df,  plot_title)\n",
        "    msg=' if you trimmed the data set it may still not be balanced or if it is balanced it may not have an adequate number of images.'\n",
        "    print_in_color(msg)\n",
        "    msg='if you want to balance the dataset or you want to create more images in each class enter \\n the number of images you want in each class if not press enter'\n",
        "    ans=input(msg)\n",
        "    if ans != '':\n",
        "        n=int(ans)\n",
        "        train_df=balance(train_df, n, 'labels', working_dir, img_size)\n",
        "        plot_title='Images per Label after manually balancing the train data set'\n",
        "        plot_label_count (train_df,  plot_title)\n",
        "    return train_df"
      ],
      "metadata": {
        "id": "R2-t1XqrPJHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(train_df,img_size, max_samples, min_samples, working_dir):\n",
        "    msg=' enter A  to auto balance the train set or enter \\n M to manually balance or hit enter to leave train set unchanged'\n",
        "    ans=input(msg)\n",
        "    if ans == 'A' or ans =='a':\n",
        "       train_df = auto_balance(train_df,img_size, max_samples, min_samples,  working_dir)\n",
        "    elif   ans== 'M' or ans == 'm':\n",
        "        train_df=manual_balance(train_df,img_size, max_samples, min_samples, working_dir)\n",
        "    else:\n",
        "        msg=f'training data set will be used as is '\n",
        "        print_in_color (msg)\n",
        "    classes=list(train_df['labels'].unique())\n",
        "    class_count = len(classes)\n",
        "    return train_df, img_size, classes, class_count"
      ],
      "metadata": {
        "id": "Bl61l933PJU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dataset_size(train_dir):\n",
        "    classes=sorted(os.listdir(train_dir))\n",
        "    ftotal=0\n",
        "    flargest=0\n",
        "    for klass in classes:\n",
        "        classpath=os.path.join(train_dir,klass)\n",
        "        if os.path.isdir(classpath):\n",
        "            flist=os.listdir(classpath)\n",
        "            fcount=len(flist)\n",
        "            if fcount>flargest:\n",
        "                flargest=fcount\n",
        "                maxclass=klass\n",
        "            ftotal += fcount\n",
        "    return ftotal, flargest, maxclass"
      ],
      "metadata": {
        "id": "KSG0iRwZPJjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg='enter the full path to the working directory where data will be stored.'\n",
        "working_dir=input(msg)\n",
        "if 'kaggle' in working_dir:\n",
        "    delimiter='/'\n",
        "else:\n",
        "    delimiter='\\\\'\n",
        "msg='Enter the full path to the train directory'\n",
        "train_dir =input(msg)\n",
        "msg = 'Enter the full path to the validation directory. If there is no validation directory press enter'\n",
        "ans=input(msg)\n",
        "if ans == '':\n",
        "    valid_dir=None\n",
        "else:\n",
        "    valid_dir=ans\n",
        "msg = 'Enter the full path to the test directory. If there is no test directory press enter'\n",
        "ans=input(msg)\n",
        "if ans == '':\n",
        "    test_dir=None\n",
        "else:\n",
        "    test_dir=ans\n",
        "ftotal, flargest, maxclass = check_dataset_size(train_dir)\n",
        "msg1= f' the train directory contains {ftotal} files, class {maxclass} has the most images of {flargest} files\\n '\n",
        "msg2='When dealing with very large data sets to save time you may not want to read in all the image files\\n'\n",
        "msg3 ='to limit the maximum number of image files in any class you can enter a limiter value'\n",
        "msg=msg1 +msg2 + msg3\n",
        "print_in_color(msg)\n",
        "msg = 'input a limiter integer value to limit max number of images in a class, for no limit hit enter'\n",
        "limiter=input(msg)\n",
        "if limiter == '':\n",
        "    limiter = None\n",
        "else:\n",
        "    limiter=int(limiter)\n",
        "    msg=f'images will be limited to a maximum of {limiter} images in each class'\n",
        "    print_in_color(msg)\n",
        "train_df, test_df, valid_df, classes, class_count, max_samples, min_samples, have, wave=make_dataframes(train_dir,test_dir, valid_dir, limiter)\n",
        "msg='Look at the print out to see the average images width and height.\\n then entire the image height to be used to train the model'\n",
        "img_height=int(input(msg))\n",
        "msg=' Enter the image width to be used to train the model'\n",
        "img_width=int(input(msg))\n",
        "img_size=(img_height, img_width)\n",
        "msg=f'model will be trained with image shape of  ( {img_height}, {img_width} )'\n",
        "print_in_color(msg)\n",
        "train_df, img_size, classes, class_count = preprocess_dataset(train_df,img_size, max_samples, min_samples, working_dir)\n",
        "train_gen, test_gen, valid_gen, test_steps= make_gens(20, 'labels', train_df, test_df, valid_df, img_size)\n",
        "show_image_samples(train_gen )\n",
        "model = make_model(img_size, .001, mod_num=3)\n",
        "epochs=100\n",
        "ask_epoch=10\n",
        "ask=LR_ASK(model, epochs=epochs,  ask_epoch=ask_epoch)\n",
        "callbacks=[ask]\n",
        "msg=f'Your model will start training. After 10 epochs you will be asked if you wish to halt training by entering H\\nor enter the number of additional epochs to run, then you will be queired again after those epochs complete.\\n To decide look at the % decrease in validation loss if it is less than 2% enter H to halt training\\n Note your model is set with the weights from the epoch with lowest validation loss'\n",
        "print_in_color (msg)\n",
        "history=model.fit(x=train_gen,   epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,\n",
        "               validation_steps=None,  shuffle=True,  initial_epoch=0)\n",
        "tr_plot(history)\n",
        "errors, tests, error_list, error_pred_list, f1score =predictor(test_gen)\n",
        "print_errors(error_list, delimiter)\n",
        "show_misclassification(error_list, error_pred_list, test_gen, delimiter)\n",
        "msg=f'your trained model will be saved to directory {working_dir} enter a subject for the saved model'\n",
        "subject=input(msg)\n",
        "save_model(subject, classes, img_size, f1score, working_dir)\n",
        "msg='model save nomenclature is directory/subject-number of classes- (img_height, img_width)- F1score.h5'\n",
        "print_in_color(msg)"
      ],
      "metadata": {
        "id": "-r9jcw_YPd62",
        "outputId": "caddd5a2-ab06-4b25-923e-c9e115169a7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0592ca7b7e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'enter the full path to the working directory where data will be stored.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mworking_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'kaggle'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworking_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "X-XOYfgv4aBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "id": "lOz4vCWUwekT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68bda699-0a54-45c8-fe59-911175a0f7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "yNOR32K_4_Rb"
      }
    }
  ]
}